{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GC4 Phase 1**\n",
    "\n",
    "Name: Julio Putra David\n",
    "\n",
    "Batch: 10\n",
    "\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "This notebook will cover the establishment of unsupervised machine learning model for a customer segmentation to define marketing strategy. The Dataset contains the usage behavior of about 9000 active credit card holders during the last 6 months.\n",
    "\n",
    "#### **Attribute Information**\n",
    "\n",
    "* `CUSTID` : Identification of Credit Card holder (Categorical)\n",
    "* `BALANCE` : Balance amount left in their account to make purchases\n",
    "* `BALANCEFREQUENCY` : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "* `PURCHASES` : Amount of purchases made from account\n",
    "* `ONEOFFPURCHASES` : Maximum purchase amount done in one-go\n",
    "* `INSTALLMENTSPURCHASES` : Amount of purchase done in installment\n",
    "* `CASHADVANCE` : Cash in advance given by the user\n",
    "* `PURCHASESFREQUENCY` : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "* `ONEOFFPURCHASESFREQUENCY` : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "* `PURCHASESINSTALLMENTSFREQUENCY` : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "* `CASHADVANCEFREQUENCY` : How frequently the cash in advance being paid\n",
    "* `CASHADVANCETRX` : Number of Transactions made with \"Cash in Advanced\"\n",
    "* `PURCHASESTRX` : Numbe of purchase transactions made\n",
    "* `CREDITLIMIT` : Limit of Credit Card for user\n",
    "* `PAYMENTS` : Amount of Payment done by user\n",
    "* `MINIMUM_PAYMENTS` : Minimum amount of payments made by user\n",
    "* `PRCFULLPAYMENT` : Percent of full payment paid by user\n",
    "* `TENURE` : Tenure of credit card service for user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "\n",
    "# Pre-processing Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from feature_engine.outliers import Winsorizer, OutlierTrimmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score,silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "# Clustering Libraries\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Warning Neglect Library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CC_GENERAL.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df.shape[0]\n",
    "c = df.shape[1]\n",
    "print('Number of rows    =', r)\n",
    "print('Number of columns =', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. List of numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = data.select_dtypes(include=np.number).columns.tolist()\n",
    "num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. List of Categorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CUST_ID` will be dropped away because it does not show any pattern for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no oddity in the descriptive statistics of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Missing Values Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several missing values in `CREDIT_LIMIT`, `MINIMUM_PAYMENTS`, and `TENURE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Distribution of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to check the distribution of the columns\n",
    "\n",
    "def skew_check(data, column):\n",
    "    skewness = data[column].skew(axis = 0, skipna = True)\n",
    "    if skewness <= 0.5 and skewness >= -0.5:\n",
    "        print(f'[Gaussian] Skewness of {column} =', skewness)\n",
    "    else:\n",
    "        print(f'[Skewed] Skewness of {column} =', skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in num_data:\n",
    "    skew_check(data, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "for i in range(0,len(num_data)):\n",
    "    plt.subplot(4,5, i+1)\n",
    "    sns.histplot(x=data[num_data[i]], color='green')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all columns are skewed. Only `PURCHASE_FREQUENCY` that has normal distribution.\n",
    "\n",
    "From the histogram, it seems that `TENURE` is a categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Boxplot Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "for i in range(0,len(num_data)):\n",
    "    plt.subplot(4,5, i+1)\n",
    "    sns.boxplot(x=data[num_data[i]], color='green')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many outliers in several columns. It will be handled later on in the Data Pre-processing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Tenure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.TENURE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tenure` is a categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8. Pearson Correlation Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised learning, we will check the multicollinearity of each feature simultaneously using Variance Inflation Factor (VIF). Now we want to see first the correlation between features using the Pearson Correlation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='Blues')\n",
    "plt.title('Pearson Correlation Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap above it appears that some features have strong and also weak correlation and with each other. Strong correlation means the feature correlate well with other feature. On the other hand, in unsupervised learning we don't want the features to have strong correlation with other features because we don't want to input features that have similar characteristic with each other so that our model would be lighter and simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Dropping Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in EDA, we will drop `CUST_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('CUST_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Inference Dataset**\n",
    "\n",
    "We will take 10 samples out to be our data inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf = data.sample(10, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.drop(data_inf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent unwanted error, we will reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_inf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to detect how many outliers in each column\n",
    "\n",
    "def detect_otl(data, column):\n",
    "    skewness = data[column].skew(axis = 0, skipna = True)\n",
    "    if skewness <= 0.5 and skewness >= -0.5:\n",
    "        upper_boundary = data[column].mean() + 1.5 * data[column].std()\n",
    "        lower_boundary = data[column].mean() - 1.5 * data[column].std()\n",
    "        print(f'[Gaussian] Skewness of {column} =', skewness)\n",
    "        print('% above upper boundary : {}'.format(len(data[data[column] > upper_boundary]) / len(data) * 100))\n",
    "        print('% below lower boundary : {}'.format(len(data[data[column] < lower_boundary]) / len(data) * 100))\n",
    "        print('-'*75)\n",
    "    else:\n",
    "        IQR = data[column].quantile(0.75) - data[column].quantile(0.25)\n",
    "        lower_boundary = data[column].quantile(0.25) - (IQR * 1.5)\n",
    "        upper_boundary = data[column].quantile(0.75) + (IQR * 1.5)\n",
    "        print(f'[Skewed] Skewness of {column} =', skewness)\n",
    "        print('% above upper boundary : {}'.format(len(data[data[column] > upper_boundary]) / len(data) * 100))\n",
    "        print('% below lower boundary : {}'.format(len(data[data[column] < lower_boundary]) / len(data) * 100))\n",
    "        print('-'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_data:\n",
    "    detect_otl(data_train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the features whose outliers will be capped and trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otl_cap = ['MINIMUM_PAYMENTS', 'PAYMENTS', 'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'CASH_ADVANCE_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n",
    "           'CASH_ADVANCE', 'INSTALLMENTS_PURCHASES', 'ONEOFF_PURCHASES', 'PURCHASES', 'BALANCE']\n",
    "otl_trim = ['CREDIT_LIMIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorizer = Winsorizer(capping_method='iqr', # We use IQR because all of the features are skewed\n",
    "                        tail='both',\n",
    "                        fold=1.5,\n",
    "                        variables=otl_cap,\n",
    "                        missing_values='ignore')\n",
    "\n",
    "data_train_capped = winsorizer.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_trimmer = OutlierTrimmer(capping_method='iqr', # We use IQR because all of the features are skewed\n",
    "                                 tail='both', \n",
    "                                 fold=1.5, \n",
    "                                 variables=otl_trim, \n",
    "                                 missing_values='ignore')\n",
    "\n",
    "data_train_trimmed = outlier_trimmer.fit_transform(data_train_capped)\n",
    "print('Size dataset - Before trimming : ', data_train_capped.shape)\n",
    "print('Size dataset - After trimming  : ', data_train_trimmed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4 Handling Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will check in which column the null values lies in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_trimmed.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have null values in `CREDIT_LIMIT` and `MINIMUM_PAYMENTS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do imputation on all occurrences of missing values with:\n",
    "* **Mean** : if the variable has a **Normal/Gaussian distribution**.\n",
    "* **Median** : if the variable has a **skewed distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na_num(data, variable):\n",
    "    skewness = data[variable].skew(axis = 0, skipna = True)\n",
    "    if skewness <= 0.5 and skewness >= -0.5:\n",
    "        data[variable].fillna(data[variable].mean(), inplace=True)\n",
    "    else:\n",
    "        data[variable].fillna(data[variable].median(), inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['CREDIT_LIMIT', 'MINIMUM_PAYMENTS']:\n",
    "    data_train_ipt = impute_na_num(data_train_trimmed, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ipt.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ipt.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5 Multicollinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the correlation between features using VIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = data_train_ipt.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(data_train_ipt.values, i) for i in range(len(data_train_ipt.columns))]\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF less than 5: `PRC_FULL_PAYMENT`, `CREDIT_LIMIT`, `PAYMENTS`\n",
    "\n",
    "We are expecting to have 4 features after PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.6 Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TENURE` is a categorical data, thus we will exclude it from feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_scaling = data_train_ipt.copy()\n",
    "data_for_scaling.drop('TENURE', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "data_train_scaled = standard_scaler.fit_transform(data_for_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled_df = pd.DataFrame(data_train_scaled, columns=data_for_scaling.columns)\n",
    "data_train_scaled_df['TENURE'] = data_train_ipt['TENURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_final = data_train_scaled_df.to_numpy()\n",
    "data_train_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.7 PCA (Principal Component Analysis)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take 80% information of the whole dataset. So we will use `n_components = 0.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=0.8).fit(data_train_final)\n",
    "pca_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Explained Variance ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,figsize=(16,5))\n",
    "ax[0].plot(range(1,7),pca_1.explained_variance_ratio_)\n",
    "ax[0].set_xlabel('Component')\n",
    "ax[0].set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "ax[1].plot(range(1,7),np.cumsum(pca_1.explained_variance_ratio_))\n",
    "ax[1].set_xlabel('Number of Component')\n",
    "ax[1].set_ylabel('Cummulative Explained Var Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cummulative EVR shows that 80% informations needs 6 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a variable that contains 80% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_reduced = pca_1.transform(data_train_final)\n",
    "data_train_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis above, it appears that 80% information needs 6 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(range_n_clusters,X):\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(15, 4)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Kmeans Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.1.1 Finding The Best Number of Cluster for Kmeans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=[2,3,4,5,6,7,8]\n",
    "inertia=[KMeans(n_clusters=i).fit(data_train_reduced).inertia_ for i in K]\n",
    "plt.plot(K,inertia, marker='o', color='r')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to determine where the elbow is when the plot is bending smoothly like the plot above. Therefore, we will check the silhouette score and plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Siluet Score & Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=[2,3,4,5,6,7,8]\n",
    "s_score=[silhouette_score(data_train_reduced, KMeans(n_clusters=i).fit(data_train_reduced).labels_) for i in K]\n",
    "plt.plot(K,s_score, marker='o', color='g')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette(K, data_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest silhouette score is on `n_cluster = 4`. However, if we see the visualization on the right side, it did not show a good clustering since we could see the green-colored cluster is mixed with the blue and yellow clusters. As well as other clusters, some clusters overlap with other clusters. \n",
    "\n",
    "The cluster that show a good visualization is `n_cluster = 2` and `n_cluster = 3`.  `n_cluster = 3` has higher silhouette score of 0.26 and it is higher than `n_cluster = 2` that has a silhouette score of 0.25. Therefore, for Kmeans Clustering will use `n_cluster = 3` to define the Kmeans Clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define Kmeans with `n_cluster = 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.1.2 Kmeans Clustering Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.1 Finding The Best Number of Cluster for Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best cluster for Agglomerative Clustering, we will use dendogram and choose the longest lines that divide the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Dendogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(data_train_reduced, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dendogram above, we will use `n_cluster = 5` because there are 5 lines between 100 and 150 that divide the data into 5 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.2 Agglomerative Clustering Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo = AgglomerativeClustering(n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Gaussian Mixture Model (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.3.1 Finding The Best Number of Component for GMM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best n_component (k) for GMM, we will calculate AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion). To determine the best `k` for GMM we will choose `k` with the lowest AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GMM with Various Number of Clusters\n",
    "\n",
    "gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(data_train_reduced)\n",
    "             for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxMBS1ORkQil",
    "outputId": "dc2ad760-2037-4ebe-ac52-251b0c632368"
   },
   "outputs": [],
   "source": [
    "# Get BIC and AIC Scores\n",
    "\n",
    "bics = [model.bic(data_train_reduced) for model in gms_per_k]\n",
    "aics = [model.aic(data_train_reduced) for model in gms_per_k]\n",
    "\n",
    "for k in range(0, 10):\n",
    "  print('Cluster : ', k+1, '\\tBIC : ', bics[k], '\\tAIC : ', aics[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BIC Score and AIC Score\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\n",
    "plt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Information Criterion\", fontsize=14)\n",
    "plt.axis([1, 11, np.min(aics) - 50, np.max(aics) + 50])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, it appears that the `k` with the lowest AIC is **10**. However, in GMM we should also choose the best `covariance_type`, thus we will search for the best combination of values for both the `n_component` and `covariance_type` hyperparameter.\n",
    "\n",
    "Since we can't use `GridSearchCV` for unsupervised learning, we will do 'for looping' to search for the best values of each hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_aic = np.infty\n",
    "\n",
    "for k in range(1, 11):\n",
    "    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n",
    "        aic = GaussianMixture(n_components=k, n_init=10,\n",
    "                              covariance_type=covariance_type,\n",
    "                              random_state=42).fit(data_train_reduced).aic(data_train_reduced)\n",
    "        if aic < min_aic:\n",
    "            min_aic = aic\n",
    "            best_k = k\n",
    "            best_covariance_type = covariance_type\n",
    "\n",
    "print('best - n_components    : ', best_k)\n",
    "print('best - covariance_type : ', best_covariance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the above plot, we get the best `n_component = 10`. For the `covariance_type`, the best value is `full`. According to Gaussian Mixture's documentation in scikit-learn.org, `full` means each component has its own general covariance matrix and it is the default `covariance_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.3.2 GMM Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=10, n_init=10, random_state=42, covariance_type='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1 Kmeans Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the Kmeans Clustering with the data that has been reduced to 80% by using `PCA(n_components=0.8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(data_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the clusters in Kmeans Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_kmeans = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Kmeans Clustering only took 8 iterations to complete the modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.2 Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the Agglomerative Clustering with the data that has been reduced to 80% by using `PCA(n_components=0.8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo.fit(data_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the clusters in Agglomerative Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_agglo = agglo.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3 Gaussian Mixture Model (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the GMM with the data that has been reduced to 80% by using `PCA(n_components=0.8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.fit(data_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the clusters in GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_gm = gm.predict(data_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the algorithm already converged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyPU1IgpkQie",
    "outputId": "4d86acec-1ae6-413e-e57b-b2084c26b173"
   },
   "outputs": [],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is converged, it means the model is already satisfied at a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-4J4Xk1kQif",
    "outputId": "1b142bf8-ccac-4b42-c89b-7e4d3461b058"
   },
   "outputs": [],
   "source": [
    "# Display Number of Step used to Reach the Convergence\n",
    "\n",
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GMM took 59 iterations to reach the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will make reduce the dimension of `data_train_final` into 2 columns using `PCA(n_components=2)` and make a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction to 2 columns\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(data_train_final)\n",
    "pca_2_tf = pca_2.transform(data_train_final)\n",
    "\n",
    "# Make a dataframe\n",
    "data_pca_df = pd.DataFrame(data = pca_2_tf, columns = ['PC 1', 'PC 2'])\n",
    "data_pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.1 Kmeans Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.1.1 Kmeans Clustering Visualization with PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate `data_pca_df` with `clusters_kmeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca_kmeans = data_pca_df.copy()\n",
    "data_pca_kmeans['clusters'] = clusters_kmeans\n",
    "data_pca_kmeans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"PC 1\", y=\"PC 2\",\n",
    "    hue=\"clusters\",\n",
    "    edgecolor='green',\n",
    "    linestyle='--',\n",
    "    data=data_pca_kmeans,\n",
    "    palette='bright',\n",
    "    ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a good clustering. We can see that all three clusters are quite well separated, although we can still see some data points that crossed other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.1.2 EDA for Kmeans Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the mean and median of each column based on the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_kmeans = data_train_ipt.copy()\n",
    "data_clustering_kmeans['CLUSTERS'] = clusters_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_kmeans.groupby('CLUSTERS').agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in `BALANCE`, `PAYMENTS`, and `CREDIT_LIMIT ` the clusters have a fairly far and reasonable distance between the clusters. Based on these clusters, we can do customer segmentations to define marketing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_kmeans[['CLUSTERS']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing from the `CREDIT_LIMIT`, there are 2227 customers in cluster 0 whose average credit limit is 5101.29, 4450 customers in cluster 1 whose average credit limit is 2937.98, and 2016 customers in cluster 2 whose average credit limit is 5781.41.\n",
    "\n",
    "Customers that are in cluster 0 has low credit limit and low balance as well. It makes sense because if we see the pearson correlation coefficient in the EDA section, we can see that `CREDIT_LIMIT` has a strong correlation with `BALANCE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_kmeans[['PURCHASES_FREQUENCY', 'CLUSTERS']].groupby('CLUSTERS').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.2 Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.2.1 Agglomerative Clustering Visualization with PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate `data_pca_df` with `clusters_agglo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca_agglo = data_pca_df.copy()\n",
    "data_pca_agglo['clusters'] = clusters_agglo\n",
    "data_pca_agglo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(15,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"PC 1\", y=\"PC 2\",\n",
    "    hue=\"clusters\",\n",
    "    edgecolor='green',\n",
    "    linestyle='--',\n",
    "    data=data_pca_agglo,\n",
    "    palette='bright',\n",
    "    ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering result above is not really good. As we can see above cluster 3 are spread all over the data points, same goes for cluster 4. We cannot really decide which data lies in which clusters because it is too overlapping and confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.1.2 EDA for Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_agglo = data_train_ipt.copy()\n",
    "data_clustering_agglo['CLUSTERS'] = clusters_agglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_agglo.groupby('CLUSTERS').agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_agglo[['CLUSTERS']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 clusters generated by the Agglomerative Clustering. We have most customers in cluster 0, which shows a moderate `BALANCE` and `CREDIT_LIMIT`. And we have least customers in cluster 3, which shows low `BALANCE` and `CREDIT_LIMIT`. \n",
    "\n",
    "This clustering is not really reliable because we can see from the plot above the clusters are not well separated and very overlapping with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.3 Gaussian Mixture Model (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.2.1 GMM Visualization with PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate `data_pca_df` with `clusters_gm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca_gm = data_pca_df.copy()\n",
    "data_pca_gm['clusters'] = clusters_gm\n",
    "data_pca_gm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(figsize=(15,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"PC 1\", y=\"PC 2\",\n",
    "    hue=\"clusters\",\n",
    "    edgecolor='green',\n",
    "    linestyle='--',\n",
    "    data=data_pca_gm,\n",
    "    palette='bright',\n",
    "    ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is very confusing. We really cannot see the boundaries of each cluster. The clusters are very overlapping and confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.1.2 EDA for GMM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the mean and median of each column based on the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_gm = data_train_ipt.copy()\n",
    "data_clustering_gm['CLUSTERS'] = clusters_gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_gm.groupby('CLUSTERS').agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_gm[['CLUSTERS']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 clusters generated by GMM. It is quite too many for doing segmentation with such number of clusters. As we can see in the table above, all mean and median in `BALANCE` and `CREDIT_LIMIT` did not differ that much, so we cannot really see the difference of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model inference, we will use Kmeans Clustering, because it showed the best clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_data:\n",
    "    detect_otl(data_inf, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_otl_cap = ['MINIMUM_PAYMENTS', 'PAYMENTS', 'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'CASH_ADVANCE_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', \n",
    "               'CASH_ADVANCE', 'INSTALLMENTS_PURCHASES', 'PURCHASES', 'BALANCE', 'PRC_FULL_PAYMENT', 'CREDIT_LIMIT' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorizer = Winsorizer(capping_method='iqr',\n",
    "                        tail='both',\n",
    "                        fold=1.5,\n",
    "                        variables=inf_otl_cap,\n",
    "                        missing_values='ignore')\n",
    "\n",
    "data_inf_capped = winsorizer.fit_transform(data_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf_capped.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude `TENURE` from feature scaling because it is a categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_for_scaling = data_inf_capped.copy()\n",
    "inf_for_scaling.drop('TENURE', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "data_inf_scaled = standard_scaler.fit_transform(inf_for_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf_scaled_df = pd.DataFrame(data_inf_scaled, columns=inf_for_scaling.columns)\n",
    "data_inf_scaled_df['TENURE'] = data_inf_capped['TENURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf_final = data_inf_scaled_df.to_numpy()\n",
    "data_inf_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PCA for Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our kmeans model is already fit with 6 features, so for PCA in the inference model we directly use `PCA(n_components = 6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_inf = PCA(n_components=6).fit_transform(data_inf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_inf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_cluster = kmeans.predict(pca_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_inf = data_inf_capped.copy()\n",
    "data_clustering_inf['CLUSTERS'] = inf_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustering_inf.groupby('CLUSTERS').agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with the train model, the clustering in Model Inference shows a good result. We can see the difference between each cluster and do the segmentation well. However, we can't really visualize this model inference because the size of the data is too small, we won't be able to see the clusters clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Conclusion**\n",
    "\n",
    "In this case, Kmeans Clustering showed better clustering result than Agglomerative Clustering and Gaussian Mixture Model. The clusters have clear boundaries, we can really see the difference of each cluster. \n",
    "\n",
    "For Agglomerative Clustering, in this case it did not perform well. The clustering result was confusing and it is quite difficult to see the boundaries of each cluster. Same goes for Gaussian Mixture Model, we could not really see the boundaries of each cluster and it is so hard to understand the clusters.\n",
    "\n",
    "For further busines purposes, it is recommended to use Kmeans Clustering to do a customer segmentation. We already have 3 clusters, and based on the `CREDIT_LIMIT` we can segment the customers with low, moderate, and high credit limits. Based on this segmentation, we can start to plan our marketing strategy. We can also segment the customers based on the other behavioral variables, such as `BALANCE`, `PURCHASE`, etc."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
